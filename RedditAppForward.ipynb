{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mV31tq5d9xVX"
   },
   "source": [
    "# Keys for Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "0tdIhqQbr5qM",
    "outputId": "a6b26e77-9b3e-428a-e403-9cbd2c43f76d"
   },
   "outputs": [],
   "source": [
    "!pip -q install praw\n",
    "import warnings\n",
    "import praw\n",
    "import pandas as pd\n",
    "from secret import client_id,client_secret,password,user_agent,username,Data1,data1csv,Data2,data2csv,Data3,data3csv,Data4,data4csv,Data5,data5csv\n",
    "                    \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id,\n",
    "                     client_secret=client_secret,\n",
    "                     password=password,\n",
    "                     user_agent=user_agent,\n",
    "                     username=username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkVTrhdV98k1"
   },
   "source": [
    "# NPL and ChatBot Lybraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "id": "qUhTYsguSziQ",
    "outputId": "7640beee-6728-4327-c4b5-2b12236cf34f"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip -q install python-telegram-bot\n",
    "!pip -q install --upgrade pip\n",
    "from telegram.ext import CommandHandler, MessageHandler, Filters, Updater\n",
    "\n",
    "\n",
    "!pip -q install -U ipykernel\n",
    "import logging\n",
    "import nltk\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download('stopwords',quiet=True)\n",
    "nltk.download('wordnet',quiet=True)\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOmaPmME-L3X"
   },
   "source": [
    "# NPL Data Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cy4laFrx8KpF"
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_toeknized_text(text):\n",
    "    other=[wordnet.lemmatize(w) for w in text ]\n",
    "    return other\n",
    "\n",
    "def stopwords_cleaner(text):\n",
    "  stoped = stopwords.words('english')\n",
    "  content = [w for w in text if w.lower() not in stoped]\n",
    "  return content\n",
    "\n",
    "\n",
    "def get_lemma(word):\n",
    "  return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "\n",
    "def prepare_text(text):\n",
    "  text=TweetTokenizer().tokenize(text)\n",
    "  text=stopwords_cleaner(text)\n",
    "  text=lemmatize_toeknized_text(text)\n",
    "  text=[get_lemma(w) for w in text]\n",
    "  thematik=''\n",
    "  for w in text:\n",
    "    thematik=thematik+'+'+w\n",
    "  thematik = thematik[1:]\n",
    "  return thematik\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80t3Auxh-Ho8"
   },
   "source": [
    "# Data Set Constrution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzr3rCl8sL_1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def redditDataMine(thematik):\n",
    "  \n",
    "  posts = []\n",
    "  Y=[thematik]\n",
    "  for post in reddit.subreddit(thematik).hot(limit=10000):\n",
    "      posts.append([post.subreddit, post.title])\n",
    "  posts = pd.DataFrame(posts,columns=['subreddit', 'title'])\n",
    "  #return str(posts.iloc[0]['subreddit'])\n",
    "  alltext='title: '+str(posts.iloc[0]['subreddit'])+ ' Thematik: '+ thematik \n",
    "  return posts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTIfdPcj-Pk1"
   },
   "source": [
    "# Keep DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "8eoUk1hj9B2L",
    "outputId": "f8fed0cc-2795-4a26-9985-cf5edd4fa3e4"
   },
   "outputs": [],
   "source": [
    "#dataDataScience = redditDataMine('DataScience')\n",
    "redditDataMine(Data1).to_csv(data1csv)\n",
    "\n",
    "redditDataMine(Data2).to_csv(data2csv)\n",
    "\n",
    "redditDataMine(Data3).to_csv(data3csv)\n",
    "\n",
    "redditDataMine(Data4).to_csv(data4csv)\n",
    "\n",
    "redditDataMine(Data5).to_csv(data5csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MACut8b0jE85"
   },
   "source": [
    "# Upload Pandas DataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "ENuhvYvheHFN",
    "outputId": "b4ba54e1-226d-4594-8e47-0062b1ed5ee0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataFrame1  = pd.read_csv(data1csv, header = 0)\n",
    "dataFrame2  = pd.read_csv(data2csv, header = 0)\n",
    "dataFrame3  = pd.read_csv(data3csv, header = 0)\n",
    "dataFrame4  = pd.read_csv(data4csv, header = 0)\n",
    "dataFrame5  = pd.read_csv(data5csv, header = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "frames = [dataFrame1, dataFrame2, dataFrame3, dataFrame4, dataFrame5]\n",
    "alldata=pd.concat(frames)\n",
    "\n",
    "alldata_data = alldata.values[:, 2]\n",
    "alldata_tarjet = alldata.values[:, 1]\n",
    "#alldata_tarjet = alldata.values[:, 2]\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train=vectorizer.fit_transform(alldata_data)\n",
    "\n",
    "############\n",
    "examples=['trump','obama','python','data','regression','university']\n",
    "examples_target=['politics','politics','datascience','datascience','datascience','education']\n",
    "#############\n",
    "x_test=vectorizer.transform(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier Accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "treeCl = tree.DecisionTreeClassifier()\n",
    "treeCl.fit(x_train, alldata_tarjet)\n",
    "predictionTree=treeCl.predict(x_test)\n",
    "#print('DecisionTreeClassifier Vektor: ', predictionTree)\n",
    "print('DecisionTreeClassifier Accuracy: ', metrics.accuracy_score(predictionTree,examples_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "zVZ3aZDRF5fm",
    "outputId": "14f3c5ae-99da-4afa-ad96-af03b9dc4705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(x_train, alldata_tarjet)\n",
    "predictionNB=clf.predict(x_test)\n",
    "#print('MultinomialNB Vektor: ', predictionNB,'\\n','='*20)\n",
    "print('MultinomialNB Accuracy: ', metrics.accuracy_score(predictionNB,examples_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import validation_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_train,alldata_tarjet)\n",
    "predictionKNN=knn.predict(x_test)\n",
    "metrics.accuracy_score(predictionKNN,examples_target)\n",
    "#print('KNN Vektor: ', predictionKNN)\n",
    "print('KNN Accuracy: ', metrics.accuracy_score(predictionKNN,examples_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "Nn=MLPClassifier()\n",
    "Nn.fit(x_train,alldata_tarjet)\n",
    "predictionNn=Nn.predict(x_test)\n",
    "metrics.accuracy_score(predictionNn,examples_target)\n",
    "#print('Nn Vektor: ', predictionNn)\n",
    "print('MLPC Accuracy: ', metrics.accuracy_score(predictionNn,examples_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc =LinearSVC()\n",
    "svc.fit(x_train,alldata_tarjet)\n",
    "predictionSvc=svc.predict(x_test)\n",
    "metrics.accuracy_score(predictionSvc,examples_target)\n",
    "#print('Nn Vektor: ', predictionSvc)\n",
    "print('Linear Accuracy: ', metrics.accuracy_score(predictionSvc,examples_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Curves KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n=np.arange(2,50,2)\n",
    "train_scores, test_scores = validation_curve(KNeighborsClassifier(),\n",
    "                                             x_train,\n",
    "                                             alldata_tarjet,\n",
    "                                             param_name='n_neighbors',\n",
    "                                             param_range=n,\n",
    "                                             cv=5)\n",
    "plt.plot(np.mean(train_scores,axis=1))\n",
    "plt.plot(np.mean(test_scores,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lerning Curves DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "lc=learning_curve(tree.DecisionTreeClassifier(),x_train,alldata_tarjet,cv=10)\n",
    "samples, train, test = lc[0], lc[1], lc[2]\n",
    "plt.plot(samples,np.mean(train,axis=1))\n",
    "plt.plot(samples,np.mean(test,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lerning Curves MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "lc=learning_curve(MultinomialNB(alpha=.01),x_train,alldata_tarjet,cv=4,n_jobs=4)\n",
    "samples, train, test = lc[0], lc[1], lc[2]\n",
    "print(samples)\n",
    "print('='*50)\n",
    "print(train)\n",
    "print('='*50)\n",
    "print(np.mean(train,axis=1))\n",
    "\n",
    "plt.plot(samples,np.mean(train,axis=1))\n",
    "plt.plot(samples,np.mean(test,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lerning Curves KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "lc=learning_curve(KNeighborsClassifier(n_neighbors=5),x_train,alldata_tarjet,cv=5)\n",
    "samples, train, test = lc[0], lc[1], lc[2]\n",
    "print(samples)\n",
    "print(np.mean(train,axis=1))\n",
    "plt.plot(samples,np.mean(train,axis=1))\n",
    "plt.plot(samples,np.mean(test,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lerning Curves MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "lc=learning_curve(MLPClassifier(),x_train,alldata_tarjet,cv=5,n_jobs=4)\n",
    "samples, train, test = lc[0], lc[1], lc[2]\n",
    "print(samples)\n",
    "print(np.mean(train,axis=1))\n",
    "plt.plot(samples,np.mean(train,axis=1))\n",
    "plt.plot(samples,np.mean(test,axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvYc17WX-DN2"
   },
   "source": [
    "# ChatBot Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "\n",
    "def start(update, context):\n",
    "    \"\"\"Start the bot.\"\"\"\n",
    "    context.bot.send_message(\n",
    "        chat_id=update.effective_chat.id,\n",
    "        text=\"I'm a Bot to research Data Science applications from Uni Bremen \\n Link: https://www.uni-bremen.de/. \\n please writte a Topyc to search!\"\n",
    "    )\n",
    "\n",
    "\n",
    "def echo(update, context):\n",
    "    \"\"\"Echo the same received message\"\"\"\n",
    "    #predict=clf.predict(x_test)  \n",
    "    context.bot.send_message(\n",
    "        chat_id=update.effective_chat.id,\n",
    "        text='thematik: '+prepare_text(update.message.text)+' Topic: '+str(knn.predict(vectorizer.transform([prepare_text(update.message.text)]))[0])\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Launch the bot (Updater)\n",
    "    updater = Updater(\n",
    "        token=token,  \n",
    "        use_context=True\n",
    "    )\n",
    "\n",
    "    # register the handlers\n",
    "    dispatcher = updater.dispatcher\n",
    "\n",
    "    start_handler = CommandHandler('start', start)\n",
    "    dispatcher.add_handler(start_handler)\n",
    "\n",
    "    echo_handler = MessageHandler(Filters.text & (~Filters.command), echo)\n",
    "    dispatcher.add_handler(echo_handler)\n",
    "\n",
    "    # start the bot\n",
    "    updater.start_polling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RedditAppForward.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
